# Validation-inductive-de-programmes-et-de-systemes-hybrides

Recent years have shown a broad adoption of deep neural networks in safetycritical applications, typically in autonomous vehicles. A fundamental challenge remains: to ensure the safety of these learning enabled systems. In particular, neural networks are often vulnerable to small disturbances, which may lead to incorrect decisions. To address this challenge and prove that a network is free of adversarial examples (usually, in a region around a given input), recent work has started investigating the use of verication. Current veriers can be broadly classied as either complete or incomplete. Complete veriers are exact, i.e., if the verier fails to certify a network then the network is non-robust (and vice-versa). They are mostly based on Mixed Integer Linear Programming (MILP) such as e.g. [2] or SMT solvers [4]. Although precise, these can only handle networks with a small number of layers and neurons. To scale, incomplete veriers usually employ overapproximation methods and hence they are sound but may fail to prove robustness even if it holds. In this second class of methods, an approach has been recently proposed [3], allowing to prove by abstract interpretation, the robustness to disturbances for neural networks of realistic size. Variations were then proposed to improve accuracy or more generally the tradeoff between accuracy and speed [6, 8, 7, 5]. These approaches are based on the abstract domains of zonotopes and polyhedra, with different variations for the abstraction of the activation functions. Another line of work is around the tool NNV https://github.com/verivital/ nnv. It aims at closed-loop systems, but also proposes some abstraction for neural networks, such as stars, see e.g. [9](http://taylortjohnson.com/research/ tran2019fm.pdf).
